import os
import re
import pandas as pd
import numpy as np
import torch
from torch.utils.data import DataLoader, random_split, Dataset

from utils import FishSalesDataset, safe_filename
from utils import NBeatsModel, train_nbeats_model


class FishSalesDatasetFlattened(Dataset):
    """
    –ú–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è FishSalesDataset –¥–ª—è N-BEATS.

    –í–º–µ—Å—Ç–æ –≤–æ–∑–≤—Ä–∞—Ç–∞ (sequence_length, num_features) –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç
    (sequence_length * num_features,) - –ø–æ–ª–Ω–æ—Å—Ç—å—é —Ñ–ª–∞—Ç—Ç–µ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä
    """

    def __init__(self, dataframe, sequence_length=30, target_column='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ'):
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
        self.original_dataset = FishSalesDataset(
            dataframe, 
            sequence_length=sequence_length, 
            target_column=target_column
        )

        self.sequence_length = sequence_length
        self.num_features = len(self.original_dataset.features_columns)
        self.flattened_size = self.sequence_length * self.num_features

        # –ö–æ–ø–∏—Ä—É–µ–º –≤–∞–∂–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã
        self.features_columns = self.original_dataset.features_columns
        self.scaler = self.original_dataset.scaler
        self.scaled_features = self.original_dataset.scaled_features
        self.scaled_target = self.original_dataset.scaled_target

    def __len__(self):
        return len(self.original_dataset)

    def __getitem__(self, idx):
        # –ü–æ–ª—É—á–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –±–∞—Ç—á
        x_orig, y = self.original_dataset[idx]

        # –§–ª–∞—Ç—Ç–µ–Ω–∏—Ä—É–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        # x_orig –∏–º–µ–µ—Ç —Ñ–æ—Ä–º—É (sequence_length, num_features) = (30, 23)
        x_flat = x_orig.flatten()  # –ü–æ–ª—É—á–∞–µ–º —Ñ–æ—Ä–º—É (690,)

        return x_flat, y


def main():
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è N-BEATS –º–æ–¥–µ–ª–µ–π —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º–∏.
    """

    # ==========================================
    # –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø
    # ==========================================

    data_path = 'data/DATA.csv'
    models_dir = 'models_nbeats'
    os.makedirs(models_dir, exist_ok=True)

    # –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã N-BEATS
    SEQUENCE_LENGTH = 30           # Lookback window (—Å–∫–æ–ª—å–∫–æ –¥–Ω–µ–π —Å–º–æ—Ç—Ä–∏–º –Ω–∞–∑–∞–¥)
    FORECAST_HORIZON = 7           # Forecast horizon (–Ω–∞ —Å–∫–æ–ª—å–∫–æ –¥–Ω–µ–π –≤–ø–µ—Ä–µ–¥)
    NUM_STACKS = 3                 # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–µ–∫–æ–≤
    NUM_BLOCKS = 3                 # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–ª–æ–∫–æ–≤ –≤ –∫–∞–∂–¥–æ–º —Å—Ç–µ–∫–µ
    HIDDEN_LAYERS = [512, 512]     # –†–∞–∑–º–µ—Ä—ã —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ–µ–≤
    DROPOUT = 0.1                  # Dropout –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å

    BATCH_SIZE = 32
    EPOCHS = 100
    LEARNING_RATE = 0.001
    EARLY_STOPPING_PATIENCE = 10

    TRAIN_TEST_RATIO = 0.8
    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {DEVICE}")
    print(f"\n–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏:")
    print(f"  - Sequence Length (lookback): {SEQUENCE_LENGTH}")
    print(f"  - Forecast Horizon: {FORECAST_HORIZON}")
    print(f"  - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–µ–∫–æ–≤: {NUM_STACKS}")
    print(f"  - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–ª–æ–∫–æ–≤ –≤ —Å—Ç–µ–∫–µ: {NUM_BLOCKS}")
    print(f"  - –°–∫—Ä—ã—Ç—ã–µ —Å–ª–æ–∏: {HIDDEN_LAYERS}")
    print()

    # ==========================================
    # –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–•
    # ==========================================

    print("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    df = pd.read_csv(data_path)
    df['date'] = pd.to_datetime(df['date']).dt.normalize()
    df = df.dropna().reset_index(drop=True)

    nomenklatura_list = df['–ù–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä–∞'].unique()
    print(f"–ù–∞–π–¥–µ–Ω–æ {len(nomenklatura_list)} –Ω–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä\n")

    # ==========================================
    # –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô
    # ==========================================

    successful_models = 0
    failed_models = 0

    for idx, nomenklatura in enumerate(nomenklatura_list):
        print(f"[{idx+1}/{len(nomenklatura_list)}] –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è: {nomenklatura}")

        # –§–∏–ª—å—Ç—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–µ–∫—É—â–µ–π –Ω–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä—ã
        df_sub = df[df['–ù–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä–∞'] == nomenklatura].reset_index(drop=True)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö
        if len(df_sub) < 100:
            print(f"  ‚ö†Ô∏è  –°–ª–∏—à–∫–æ–º –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö ({len(df_sub)} < 100), –ø—Ä–æ–ø—É—Å–∫–∞–µ–º\n")
            failed_models += 1
            continue

        try:
            # ==========================================
            # –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–¢–ê–°–ï–¢–ê (–ò–°–ü–†–ê–í–õ–ï–ù–û)
            # ==========================================

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º FLATTENED –¥–∞—Ç–∞—Å–µ—Ç –≤–º–µ—Å—Ç–æ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ
            dataset = FishSalesDatasetFlattened(
                df_sub, 
                sequence_length=SEQUENCE_LENGTH, 
                target_column='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ'
            )

            # –ö–õ–Æ–ß–ï–í–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π input_size
            input_size = SEQUENCE_LENGTH * dataset.num_features

            print(f"  - –†–∞–∑–º–µ—Ä –≤—Ö–æ–¥–∞ (flattened): {input_size} "
                  f"({SEQUENCE_LENGTH} –¥–Ω–µ–π √ó {dataset.num_features} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)")

            if len(dataset) < 10:
                print(f"  ‚ö†Ô∏è  –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º\n")
                failed_models += 1
                continue

            # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –æ–±—É—á–∞—é—â–∏–π –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä—ã
            size = len(dataset)
            train_size = int(TRAIN_TEST_RATIO * size)
            val_size = size - train_size

            train_set, val_set = random_split(
                dataset, 
                [train_size, val_size], 
                generator=torch.Generator().manual_seed(42)
            )

            # –°–æ–∑–¥–∞–µ–º DataLoaders
            train_loader = DataLoader(
                train_set, 
                batch_size=BATCH_SIZE, 
                shuffle=True
            )
            val_loader = DataLoader(
                val_set, 
                batch_size=BATCH_SIZE, 
                shuffle=False
            )

            print(f"  - –†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞: {train_size}")
            print(f"  - –†–∞–∑–º–µ—Ä –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞: {val_size}")

            # ==========================================
            # –°–û–ó–î–ê–ù–ò–ï –ò –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò
            # ==========================================

            # –°–æ–∑–¥–∞–µ–º N-BEATS –º–æ–¥–µ–ª—å —Å –ü–†–ê–í–ò–õ–¨–ù–´–ú input_size
            model = NBeatsModel(
                input_size=input_size,              # ‚Üê –ò–°–ü–†–ê–í–õ–ï–ù–û!
                output_size=FORECAST_HORIZON,
                num_stacks=NUM_STACKS,
                num_blocks=NUM_BLOCKS,
                hidden_layers=HIDDEN_LAYERS,
                dropout=DROPOUT
            )

            print(f"  - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: "
                  f"{sum(p.numel() for p in model.parameters()):,}")

            # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
            print("  - –û–±—É—á–µ–Ω–∏–µ –∑–∞–ø—É—â–µ–Ω–æ...")
            model, history = train_nbeats_model(
                model=model,
                train_loader=train_loader,
                val_loader=val_loader,
                epochs=EPOCHS,
                lr=LEARNING_RATE,
                device=DEVICE,
                early_stopping_patience=EARLY_STOPPING_PATIENCE
            )

            # ==========================================
            # –°–û–•–†–ê–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ò
            # ==========================================

            safe_name = safe_filename(nomenklatura)

            model_path = os.path.join(models_dir, f"{safe_name}_nbeats.pth")
            scaler_path = os.path.join(models_dir, f"{safe_name}_nbeats_scaler.pkl")
            features_path = os.path.join(models_dir, f"{safe_name}_nbeats_features.npy")
            config_path = os.path.join(models_dir, f"{safe_name}_nbeats_config.txt")

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏
            torch.save(model.state_dict(), model_path)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º scaler
            import pickle
            with open(scaler_path, 'wb') as f:
                pickle.dump(dataset.scaler, f)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            np.save(features_path, np.array(dataset.features_columns, dtype=object))

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –º–æ–¥–µ–ª–∏ (–ò–°–ü–†–ê–í–õ–ï–ù–û!)
            config = {
                'input_size': input_size,           # ‚Üê –ò–°–ü–†–ê–í–õ–ï–ù–û!
                'forecast_horizon': FORECAST_HORIZON,
                'num_stacks': NUM_STACKS,
                'num_blocks': NUM_BLOCKS,
                'hidden_layers': HIDDEN_LAYERS,
                'dropout': DROPOUT,
                'sequence_length': SEQUENCE_LENGTH,
                'num_features': dataset.num_features,  # ‚Üê –ù–û–í–û–ï: –¥–ª—è –¥–µ—Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏
                'is_flattened': True                    # ‚Üê –ù–û–í–û–ï: —Ñ–ª–∞–≥
            }

            with open(config_path, 'w') as f:
                for key, value in config.items():
                    f.write(f"{key}: {value}\n")

            print(f"  ‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")
            print(f"  ‚úÖ Scaler —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {scaler_path}")
            print(f"  ‚úÖ –ü—Ä–∏–∑–Ω–∞–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {features_path}")
            print()

            successful_models += 1

        except Exception as e:
            print(f"  ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {str(e)}\n")
            failed_models += 1
            import traceback
            traceback.print_exc()
            continue

    # ==========================================
    # –ò–¢–û–ì–ò
    # ==========================================

    print("="*60)
    print(f"–û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û")
    print(f"  ‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–æ –º–æ–¥–µ–ª–µ–π: {successful_models}")
    print(f"  ‚ùå –û—à–∏–±–æ–∫ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {failed_models}")
    print(f"  üìÅ –ú–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {models_dir}")
    print("="*60)


if __name__ == "__main__":
    main()